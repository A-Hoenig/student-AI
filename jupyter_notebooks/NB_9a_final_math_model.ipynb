{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **STUDENT AI** - MATH MODEL CREATION (CLASSIFICATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "Output a usable model for predicting student performance for use in a streamlit dashboard.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "Main Data set cleaned and engineered for model training.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "Pipeline and .pkl file to use for predicting a students math score based on the derived calculated best feature variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All Libraries Loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages/xgboost/compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
            "  from pandas import MultiIndex, Int64Index\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "### Pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Feature Engineering\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "\n",
        "### Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### libraries for custom transformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "### Feature Balancing\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "### Feature  Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "### ML algorithms \n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "### EqualFrequencyDiscretiser\n",
        "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
        "\n",
        "### packages for classification report and confusion matrix\n",
        "from sklearn.metrics import make_scorer, recall_score\n",
        "\n",
        "### Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "### Packages for generating a classification report and confusion matrix\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "### GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "print('All Libraries Loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "### Set the working directory to notebook parent folder\n",
        "If the output does not match, click **'clear all outputs'** and then **'restart'** the notebook. \n",
        "Then run cells from top to bottom."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If correct, Active Directory should read: /workspace/student-AI\n",
            "Active Directory: /workspace/student-AI\n"
          ]
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "os.chdir(os.path.dirname(current_dir))\n",
        "current_dir = os.getcwd()\n",
        "print('If correct, Active Directory should read: /workspace/student-AI')\n",
        "print(f\"Active Directory: {current_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load cleaned dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_maths = pd.read_csv('outputs/dataset/Expanded_data_with_more_features_clean.csv').filter(['Gender', 'EthnicGroup', 'ParentEduc', 'LunchType', 'TestPrep', 'MathScore'])\n",
        "df_maths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "efd = EqualFrequencyDiscretiser(q=2, variables=['MathScore'])\n",
        "df_maths_efd = efd.fit_transform(df_maths)\n",
        "\n",
        "print(f\"* The classes represent the following ranges: \\n{efd.binner_dict_} \\n\")\n",
        "fig_maths_efd = sns.countplot(data=df_maths_efd, x='MathScore')\n",
        "plt.bar_label(fig_maths_efd.containers[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_maths_efd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split Data Set into Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " math_train_features, math_test_features, math_train_scores, math_test_scores = train_test_split(\n",
        "    df_maths_efd.drop(['MathScore'], axis=1),\n",
        "    df_maths_efd['MathScore'],\n",
        "    test_size = 0.2,\n",
        "    random_state = 7\n",
        ")\n",
        "\n",
        "print(\"* Train set:\", math_train_features.shape, math_train_scores.shape, \"\\n* Test set:\",  math_test_features.shape, math_test_scores.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "math_train_scores.value_counts().plot(kind='bar', title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_set = ['ParentEduc','EthnicGroup']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove all other features..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "math_train_features = math_train_features.filter(feature_set)\n",
        "math_test_features = math_test_features.filter(feature_set)\n",
        "math_test_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineOptimization(model):\n",
        "    pipeline_base = Pipeline([\n",
        "\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(\n",
        "            encoding_method='arbitrary', \n",
        "            variables=feature_set)),\n",
        "\n",
        "        (\"model\", model),\n",
        "\n",
        "    ])\n",
        "\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define best algorithm and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_model = {\"AdaBoostClassifier\": AdaBoostClassifier(random_state=0),}\n",
        "selected_model_parameters = {\"AdaBoostClassifier\": {\n",
        "    'model__n_estimators': [80],\n",
        "    'model__learning_rate':[0.1],\n",
        "    }\n",
        "  }\n",
        "\n",
        "selected_model_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final GridSearch CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, parameters):\n",
        "        self.models = models\n",
        "        self.parameters = parameters\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, x, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "            model =  PipelineOptimization(self.models[key])\n",
        "\n",
        "            parameters = self.parameters[key]\n",
        "            grid_search = GridSearchCV(model, parameters, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
        "            grid_search.fit(x, y)\n",
        "            self.grid_searches[key] = grid_search\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, parameters):\n",
        "            summary = {\n",
        "                 'estimator': key,\n",
        "                 'minimum_score': min(scores),\n",
        "                 'maximum_score': max(scores),\n",
        "                 'mean_score': np.mean(scores),\n",
        "                 'standard_deviation_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**parameters,**summary})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            parameters = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                result = self.grid_searches[k].cv_results_[key]        \n",
        "                scores.append(result.reshape(len(parameters), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(parameters, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "\n",
        "        columns = ['estimator', 'minimum_score', 'mean_score', 'maximum_score', 'standard_deviation_score']\n",
        "        columns = columns + [column for column in df.columns if column not in columns]\n",
        "\n",
        "        return df[columns], self.grid_searches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "math_model = HyperparameterOptimizationSearch(models=selected_model, parameters=selected_model_parameters)\n",
        "math_model.fit(math_train_features, math_train_scores,\n",
        "           scoring = make_scorer(recall_score, labels=[0], average=None),\n",
        "           n_jobs=-1,cv=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Confusion Matrix Function from CI Customer Churn Course"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def confusion_matrix_and_report(x, y, pipeline, label_map):\n",
        "\n",
        "  prediction = pipeline.predict(x)\n",
        "\n",
        "  print('---  Confusion Matrix  ---')\n",
        "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
        "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
        "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
        "        ))\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print('---  Classification Report  ---')\n",
        "  print(classification_report(y, prediction, target_names=label_map),\"\\n\")\n",
        "\n",
        "def clf_performance(math_train_features, math_train_scores, math_test_features, math_test_scores, pipeline, label_map):\n",
        "  print(\"#### Train Set #### \\n\")\n",
        "  confusion_matrix_and_report(math_train_features, math_train_scores, pipeline, label_map)\n",
        "\n",
        "  print(\"#### Test Set ####\\n\")\n",
        "  confusion_matrix_and_report(math_test_features, math_test_scores, pipeline, label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_summary_final, grid_search_pipelines_final = math_model.score_summary()\n",
        "grid_search_summary_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_model = grid_search_summary_final.iloc[0,0]\n",
        "final_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf_final = grid_search_pipelines_final[final_model].best_estimator_\n",
        "pipeline_clf_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_cleaning_feat_eng_steps = 1\n",
        "columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_final.steps[:data_cleaning_feat_eng_steps])\n",
        "                                        .transform(math_train_features)\n",
        "                                        .columns)\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance_final = (pd.DataFrame(data={\n",
        "          'Feature': columns_after_data_cleaning_feat_eng,\n",
        "          'Importance': pipeline_clf_final['model'].feature_importances_})\n",
        "  .sort_values(by='Importance', ascending=False)\n",
        "  )\n",
        "\n",
        "best_features_final = df_feature_importance_final['Feature'].to_list() # reassign best features in order\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features_final)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{best_features_final}\")\n",
        "\n",
        "df_feature_importance_final.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def confusion_matrix_and_report(x, y, pipeline, label_map):\n",
        "\n",
        "  prediction = pipeline.predict(x)\n",
        "\n",
        "  print('---  Confusion Matrix  ---')\n",
        "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
        "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
        "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
        "        ))\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print('---  Classification Report  ---')\n",
        "  print(classification_report(y, prediction, target_names=label_map),\"\\n\")\n",
        "\n",
        "def clf_performance(math_train_features, math_train_scores, math_test_features, math_test_scores, pipeline, label_map):\n",
        "  print(\"#### Train Set #### \\n\")\n",
        "  confusion_matrix_and_report(math_train_features, math_train_scores, pipeline, label_map)\n",
        "\n",
        "  print(\"#### Test Set ####\\n\")\n",
        "  confusion_matrix_and_report(math_test_features, math_test_scores, pipeline, label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_map = ['might need assistance', 'will not need assistance']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(math_train_features, math_train_scores, math_test_features, math_test_scores, pipeline_clf_final, label_map )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis\n",
        "This is a good verification that reducing the dataset to only the feature variables had no effect on the end result. I can go ahead and save the model for use in the dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Final Model Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Check variable content before saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# math_train_features.head()\n",
        "# math_train_scores.head()\n",
        "# math_test_features.head()\n",
        "# math_test_scores.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Model Type / Version and destination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "version = \"v1\"\n",
        "target = \"math\"\n",
        "\n",
        "file_path = f\"outputs/models/{target}/{version}\"\n",
        "file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define which files to save/update\n",
        "file_names = [\n",
        "    f\"{target}-train-features.csv\",\n",
        "    f\"{target}-train-scores.csv\",\n",
        "    f\"{target}-test-features.csv\",\n",
        "    f\"{target}-test-scores.csv\",\n",
        "    f\"{target}-model.pkl\",\n",
        "    f\"{target}-labels.pkl\",\n",
        "    f\"{target}-feature-importance.png\",\n",
        "]\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(file_path, exist_ok=True)\n",
        "\n",
        "# Remove files if they exist\n",
        "print(f'*** Removing previous files  ***')\n",
        "for name in file_names:\n",
        "    file = os.path.join(file_path, name)\n",
        "    if os.path.exists(file):\n",
        "        os.remove(file)\n",
        "        print(f\"{file} removed\")\n",
        "    else:\n",
        "        print(f\"{file} does not exist\")\n",
        "\n",
        "print('')\n",
        "\n",
        "# Save .csv files\n",
        "print(f'*** Creating files in: {file_path} ***')\n",
        "\n",
        "math_train_features.to_csv(f\"{file_path}/{target}-train-features.csv\", index=False)\n",
        "print(f'{target}-train-features.csv created')\n",
        "\n",
        "math_train_scores.to_csv(f\"{file_path}/{target}-train-scores.csv\", index=False)\n",
        "print(f'{target}-train-scores.csv created')\n",
        "\n",
        "math_test_features.to_csv(f\"{file_path}/{target}-test-features.csv\", index=False)\n",
        "print(f'{target}-test-features.csv created')\n",
        "\n",
        "math_test_scores.to_csv(f\"{file_path}/{target}-test-scores.csv\", index=False)\n",
        "print(f'{target}-test-scores.csv created')\n",
        "\n",
        "# Save .pkl model files\n",
        "joblib.dump(value=pipeline_clf_final, filename=f\"{file_path}/{target}-model.pkl\")\n",
        "print(f'{target}_model.pkl created')\n",
        "\n",
        "joblib.dump(value=label_map, filename=f\"{file_path}/{target}-labels.pkl\")\n",
        "print(f'{target}_labels created')\n",
        "\n",
        "# Save features plot image\n",
        "df_feature_importance_final.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.savefig(f'{file_path}/{target}-feature-importance.png', bbox_inches='tight')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
